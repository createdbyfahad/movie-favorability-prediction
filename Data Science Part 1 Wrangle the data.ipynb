{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>\n",
    "<center>\n",
    "Data Science Part 1: Wrangle The Data\n",
    "</center>\n",
    "</h1>\n",
    "<div class=h1_cell>\n",
    "By Fahad Alarefi. The purpose of this three-part project is to use machine learning into making use of a publicaly available table called IMDB 5000 titles.\n",
    "The main idea is to come up with a function which can predictate, with high accuracy, whether a provided movie data will have 8 stars or more by the user community. The technique of decision trees will be used.\n",
    "The dataset is accessed from here:\n",
    "https://data.world/data-society/imdb-5000-movie-dataset\n",
    "\n",
    "</div>\n",
    "<br />\n",
    "<div class=h1_cell>\n",
    "In this part, we will wrangle the data.\n",
    "Wrangling the data means to clean the dataset and make it ready for machine learning!\n",
    "We will be using Pandas library to clean the data and apply the decision trees.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>\n",
    "Read the dataset from a public url\n",
    "</h1>\n",
    "<div class=h1_cell>\n",
    "First step,\n",
    "get the dataset from the web so that we can work on it.\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "url = 'https://github.com/createdbyfahad/datascience/files/2593679/movie_metadata.txt'\n",
    "movie_table = pd.read_csv(url)\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.options.display.float_format = '{:,.0f}'.format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=h1_cell>\n",
    "Here we can print the top 5 rows from the table:\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_table.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the status of the data (find any empties).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_table.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like many parts of the data are missing,\n",
    "however, first we will delete some rows and columns then revaluate empties.\n",
    "<br />\n",
    "Since we are interested in modern movies, then first check how many non-color movies are there"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(movie_table.loc[movie_table['color'] != 'Color'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " delete all movies that are not in color"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_table = movie_table.loc[movie_table['color'] == 'Color']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to make the dataset ligher, we will eliminate all columns that we are not going to use\n",
    "<br /> \n",
    "things like the name of director or the film won't get used in our implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_columns = [\n",
    "     'color',\n",
    "     'director_name',\n",
    "     'num_critic_for_reviews',\n",
    "     'director_facebook_likes',\n",
    "     'actor_3_facebook_likes',\n",
    "     'actor_2_name',\n",
    "     'actor_1_facebook_likes',\n",
    "     'actor_1_name',\n",
    "     'movie_title',\n",
    "     'num_voted_users',\n",
    "     'cast_total_facebook_likes',\n",
    "     'actor_3_name',\n",
    "     'facenumber_in_poster',\n",
    "     'plot_keywords',\n",
    "     'movie_imdb_link',\n",
    "     'num_user_for_reviews',\n",
    "     'language',\n",
    "     'actor_2_facebook_likes',\n",
    "     'aspect_ratio',\n",
    "     'movie_facebook_likes'\n",
    "]\n",
    "movie_table = movie_table.drop(drop_columns, axis=1)\n",
    "movie_table.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now that we have all important information about the each movie, \n",
    "<br />\n",
    "we will start to prepare it for machine learning\n",
    "<br />\n",
    "first, check if how many movies have not content_rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(movie_table.loc[movie_table['content_rating'].isnull()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "since we cannot 'guess' what the movie content type, <br />\n",
    "then delete all non-rated movies\n",
    "<br /><br />\n",
    "also, delete all movies that have not date /OR duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_table = movie_table.loc[movie_table['content_rating'].notnull()]\n",
    "movie_table = movie_table.loc[movie_table['title_year'].notnull()]\n",
    "movie_table = movie_table.loc[movie_table['duration'].notnull()]\n",
    "len(movie_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we are only interested in movies, so drop all tv series<br />\n",
    "if it has \"tv\" in it's rating, then drop it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_table = movie_table[~movie_table['content_rating'].str.contains(\"TV\")]\n",
    "#movie_table = movie_table[movie_table.apply(lambda row: 'TV' not in row['content_rating'], axis=1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we are left with good amount of movie titles\n",
    "<br />\n",
    "in order to process the genres, we need to convert them into 1s and 0s\n",
    "<br >\n",
    "so we will create a column for each possible genre and fill it with 0 or 1\n",
    "<br />\n",
    "It's called one-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#movie_table['genres'] = movie_table.genres.str.split('\\s*|\\s*', expand=True).stack().str.get_dummies().sum(level=0)\n",
    "movie_table_gd = movie_table['genres'].str.get_dummies(sep='|')\n",
    "movie_table = movie_table.join(movie_table_gd)\n",
    "#movie_table += genres\n",
    "movie_table.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "drop movies with nan country, <br />\n",
    "and then one hot encode the coutnry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_table = movie_table.loc[movie_table['country'].notnull()]\n",
    "one_hot_c = pd.get_dummies(movie_table['country'],prefix='c',dummy_na=False)\n",
    "movie_table = movie_table.join(one_hot_c)\n",
    "movie_table.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "similarly, one hot encode the content_rating <br />\n",
    "also, I have noticed that some movies use the old rating system (M, GP, G, and X).\n",
    "<br /> so I will change it to the newer systm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_r = pd.get_dummies(movie_table['content_rating'],prefix='rating',dummy_na=False)\n",
    "movie_table = movie_table.join(one_hot_r)\n",
    "#movie_table['rating_PG'] = movie_table.apply(lambda row: row['rating_G'])\n",
    "movie_table['rating_PG'] = (movie_table['rating_G'] | movie_table['rating_PG'])\n",
    "movie_table['rating_PG-13'] = (movie_table['rating_GP'] | movie_table['rating_M'] | movie_table['rating_PG-13'])\n",
    "movie_table['rating_NC-17'] = (movie_table['rating_X'] | movie_table['rating_NC-17'])\n",
    "movie_table['rating_Unrated'] = (movie_table['rating_Not Rated'] | movie_table['rating_Approved'] | movie_table['rating_Unrated'] | movie_table['rating_Passed'])\n",
    "# then delete unnessasery columns\n",
    "movie_table = movie_table.drop(['rating_G', 'rating_M', 'rating_GP', 'rating_X', 'rating_Not Rated', 'rating_Approved', 'rating_Passed'], axis=1)\n",
    "movie_table.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are done with content rating, <br />\n",
    "next, we will classify the numbers data, like \n",
    "new_title = if title >= 2000\n",
    "length_avg = if duration > average movie duration (130 minutes)\n",
    "good_score = if imdb_score >= 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_table['is_new'] = movie_table.apply(lambda row: 1 if row.title_year >= 2000.0 else 0, axis=1)\n",
    "movie_table['is_long'] = movie_table.apply(lambda row: 1 if row.duration > 130.0 else 0, axis=1)\n",
    "movie_table['is_good'] = movie_table.apply(lambda row: 1 if row.imdb_score >= 8.0 else 0, axis=1)\n",
    "\n",
    "movie_table.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are some important functions that will help in applying the kmeans values\n",
    "(source: Stephen Ficaks, University of Oregon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def row_to_vect(row, features):\n",
    "    vect = []\n",
    "    for feature in features:\n",
    "        vect.append(float(row[feature]))\n",
    "    return tuple(vect)\n",
    "\n",
    "def initialize_centroids(sample_table, features):\n",
    "    k = len(sample_table)\n",
    "    centroids = {}\n",
    "    for i in range(k):\n",
    "        row = sample_table.iloc[i]\n",
    "        vector = row_to_vect(row, features)\n",
    "        centroids[i] = {'centroid': vector, 'cluster': []}\n",
    "        \n",
    "    return centroids\n",
    "\n",
    "def euclidean_distance(vect1, vect2):\n",
    "    sum = 0\n",
    "    for i in range(len(vect1)):\n",
    "        sum += (vect1[i] - vect2[i])**2\n",
    "    return sum**.5  # I claim that this square root is not needed in K-means - see why?\n",
    "\n",
    "def closest_centroid(centroids, row, k):\n",
    "    min_distance = euclidean_distance(centroids[0]['centroid'], row)\n",
    "    min_centroid = 0\n",
    "    for i in range(1,k):\n",
    "        distance = euclidean_distance(centroids[i]['centroid'], row)\n",
    "        if distance < min_distance:\n",
    "            min_distance = distance\n",
    "            min_centroid = i\n",
    "    return (min_centroid, min_distance)\n",
    "\n",
    "def phase_1(centroids, table, features, k):\n",
    "    for i in range(k):\n",
    "        centroids[i]['cluster'] = []  # starting new phase 1 so empty out values from prior iteration\n",
    "    \n",
    "    #Go through every row in Titanic table (or Loan table) and place in closest centroid cluster.\n",
    "    for i in range(len(table)):\n",
    "        row = table.iloc[i]\n",
    "        vrow = row_to_vect(row, features)\n",
    "        (index, dist) = closest_centroid(centroids, vrow, k)\n",
    "        centroids[index]['cluster'].append(vrow)\n",
    "    \n",
    "    return centroids\n",
    "\n",
    "#cluster is a list of points, i.e., a list of lists.\n",
    "def compute_mean(cluster):\n",
    "    if len(cluster) == 0:\n",
    "        return []\n",
    "    the_sum = cluster[0]  # use 0th point as starter\n",
    "    \n",
    "    #I am using zip to pair up all points then do addition\n",
    "    for i in range(1,len(cluster)):\n",
    "        the_sum = [pair[0]+pair[1] for pair in zip(the_sum, cluster[i])]\n",
    "    n = len(cluster)*1.0\n",
    "    the_mean_point = [x/n for x in the_sum]\n",
    "    return the_mean_point\n",
    "\n",
    "\n",
    "def phase_2(centroids, k, threshold):\n",
    "    old_centroids = []\n",
    "    \n",
    "    stop = True\n",
    "    #Compute k new centroids and check for stopping condition\n",
    "    for i in range(k):\n",
    "        current_centroid = centroids[i]['centroid']\n",
    "        new_centroid = compute_mean(centroids[i]['cluster'])\n",
    "        centroids[i]['centroid'] = new_centroid\n",
    "        if euclidean_distance(current_centroid, new_centroid) > threshold:\n",
    "            stop = False  # all it takes is one\n",
    "\n",
    "    return (stop, centroids)\n",
    "\n",
    "\n",
    "def k_means(table, features, k, hypers):\n",
    "    n = 100 if 'n' not in hypers else hypers['n']\n",
    "    threshold = 0.0 if 'threshold' not in hypers else hypers['threshold']\n",
    "    \n",
    "    centroid_table = table.sample(n=k, replace=False, random_state=100)  # only random choice I am making\n",
    "    centroids = initialize_centroids(centroid_table, features)\n",
    "    \n",
    "    j = 0\n",
    "    stop = False\n",
    "    while( j < n and not stop):\n",
    "        print('starting '+str(j+1))\n",
    "        centroids = phase_1(centroids, table, features, k)\n",
    "        (stop, centroids) = phase_2(centroids, k, threshold)\n",
    "        j += 1\n",
    "    print('done')\n",
    "    return centroids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(movie_table.loc[movie_table['gross'].isnull()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "gross is not so important field, but it will help us in figuring out the missing budget values\n",
    "so as you see above, gross has a lot empty values\n",
    "<br /> so using kmeans, we will fill it with values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(movie_table.loc[movie_table['duration'].isnull()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_used = [u'duration',\n",
    "        u'title_year', u'imdb_score', u'Action', u'Adventure',\n",
    "       u'Animation', u'Biography', u'Comedy', u'Crime', u'Documentary',\n",
    "       u'Drama', u'Family', u'Fantasy', u'Film-Noir', u'History', u'Horror',\n",
    "       u'Music', u'Musical', u'Mystery', u'News', u'Romance', u'Sci-Fi',\n",
    "       u'Sport', u'Thriller', u'War', u'Western', u'c_Afghanistan',\n",
    "       u'c_Argentina', u'c_Aruba', u'c_Australia', u'c_Bahamas', u'c_Belgium',\n",
    "       u'c_Brazil', u'c_Bulgaria', u'c_Cameroon', u'c_Canada', u'c_Chile',\n",
    "       u'c_China', u'c_Colombia', u'c_Czech Republic', u'c_Denmark',\n",
    "       u'c_Dominican Republic', u'c_Egypt', u'c_Finland', u'c_France',\n",
    "       u'c_Georgia', u'c_Germany', u'c_Greece', u'c_Hong Kong', u'c_Hungary',\n",
    "       u'c_Iceland', u'c_India', u'c_Indonesia', u'c_Iran', u'c_Ireland',\n",
    "       u'c_Israel', u'c_Italy', u'c_Japan', u'c_Kyrgyzstan', u'c_Mexico',\n",
    "       u'c_Netherlands', u'c_New Line', u'c_New Zealand', u'c_Norway',\n",
    "       u'c_Official site', u'c_Panama', u'c_Peru', u'c_Philippines',\n",
    "       u'c_Poland', u'c_Romania', u'c_Russia', u'c_Slovakia',\n",
    "       u'c_South Africa', u'c_South Korea', u'c_Spain', u'c_Sweden',\n",
    "       u'c_Taiwan', u'c_Thailand', u'c_UK', u'c_USA', u'c_West Germany',\n",
    "       u'rating_NC-17', u'rating_PG', u'rating_PG-13', u'rating_R',\n",
    "       u'rating_Unrated']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 5\n",
    "final_centroids = k_means(movie_table, features_used, k, {'n':100, 'threshold':0.0})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now we need to create a table that has all rows with nonempty 'gross' value\n",
    "we will use that table to get a guess the empty value in our table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "focus_col = 'gross'\n",
    "\n",
    "nonempty_gross_table = movie_table.loc[movie_table[focus_col].notnull()]\n",
    "print(len(nonempty_gross_table))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_centroid_labels(centroids, focus_table, focus_column, features, k):\n",
    "    for i in range(len(focus_table)):\n",
    "        row = focus_table.iloc[i]\n",
    "        vrow = row_to_vect(row, features)\n",
    "        (minc, mind) = closest_centroid(centroids, vrow, k)\n",
    "        if focus_column not in centroids[minc]:\n",
    "            centroids[minc][focus_column] = [row[focus_column]*1.0]\n",
    "        else:\n",
    "            centroids[minc][focus_column].append(row[focus_column]*1.0)\n",
    "    for ind in range(k): \n",
    "        if len(centroids[ind][focus_column]) == 0:\n",
    "            centroids[ind]['mean_label'] = 0.0\n",
    "        else:\n",
    "            the_sum = centroids[ind][focus_column][0]\n",
    "            for i in range(1, len(centroids[ind][focus_column])):\n",
    "                the_sum += centroids[ind][focus_column][i]\n",
    "            centroids[ind]['mean_label'] = the_sum/(len(centroids[ind][focus_column]) * 1.0)\n",
    "            \n",
    "    return centroids\n",
    "    \n",
    "centroids = compute_centroid_labels(final_centroids, nonempty_gross_table, focus_col, features_used, k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now we will fill the empty 'gross' values in the table.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kmeans_fill(centroids, full_table, features, focus_column, k):\n",
    "   \n",
    "    def get_closest_clmn(row):\n",
    "        vrow = row_to_vect(row, features)\n",
    "        (minc, mind) = closest_centroid(centroids, vrow, k)\n",
    "        \n",
    "        return centroids[minc]['mean_label']\n",
    "    \n",
    "    new_table = pd.DataFrame(full_table)\n",
    "    new_table['kmeans_'+focus_column] = new_table.apply(lambda row: get_closest_clmn(row) if pd.isnull(row[focus_column]) else row[focus_column], axis=1)\n",
    "    \n",
    "    return new_table\n",
    "\n",
    "movie_table = kmeans_fill(centroids, movie_table, features_used, focus_col, k)\n",
    "movie_table.loc[movie_table[focus_col].isnull()].head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you see above, all empty 'gross' values have kmeans value in the same row\n",
    "<br /> <br />\n",
    "now we will do the same thing for the budget column and get a kmeans estimate\n",
    "<br /> \n",
    "but since we have a 'gross' value for every movie, then we will use that info <br />\n",
    "to gain better estimate for the budget."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_used.append('kmeans_gross')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we will redo what we last did, but now try to get the budget value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "focus_col = 'budget'\n",
    "\n",
    "nonempty_budget_table = movie_table.loc[movie_table[focus_col].notnull()]\n",
    "print(len(nonempty_budget_table))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "second_final_centroids = k_means(movie_table, features_used, k, {'n':100, 'threshold':0.0})\n",
    "second_centroids = compute_centroid_labels(second_final_centroids, nonempty_budget_table, focus_col, features_used, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_table = kmeans_fill(second_centroids, movie_table, features_used, focus_col, k)\n",
    "movie_table.loc[movie_table[focus_col].isnull()].head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Now, we are going to use this data to further examination. <br />\n",
    "In the the next two parts we will try two models of prediciton. (Random Forests and Decision Trees) <br />\n",
    "In order to use reuse this data, we need to store in a file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "movie_table.to_csv(\"part1_movie_table.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
